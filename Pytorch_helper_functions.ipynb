{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONoZ7U8eRK1FIyi0aB300N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andreluis2001/Pytorch_helper_library/blob/main/Pytorch_helper_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MGp_nttXPg5",
        "outputId": "6962f70c-7189-4940-d06d-d2eb364b4db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_setup.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data_setup.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "def download_data(\n",
        "    url_source: str,\n",
        "    destination: str,\n",
        "    remove_source: bool = True\n",
        ") -> Path:\n",
        "\n",
        "  data_dir_path = Path(\"data\")\n",
        "  data_path = data_dir_path / destination\n",
        "\n",
        "  if data_path.is_dir():\n",
        "    print(f\"{data_path} already exists, skipping download\")\n",
        "\n",
        "  else:\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    target_zip_file = Path(url_source).name\n",
        "\n",
        "    with open(data_dir_path / target_zip_file, \"wb\") as f:\n",
        "\n",
        "      request = requests.get(url_source)\n",
        "      f.write(request.content)\n",
        "\n",
        "    with zipfile.ZipFile(data_dir_path / target_zip_file, \"r\") as zip_f:\n",
        "\n",
        "      zip_f.extractall(data_path)\n",
        "\n",
        "    if remove_source:\n",
        "      os.remove(data_dir_path / target_zip_file)\n",
        "\n",
        "    return data_path\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transforms: transforms.Compose,\n",
        "    batch_size: int = 32,\n",
        "    num_workers: int = 1\n",
        ") -> Tuple[\n",
        "    torch.utils.data.DataLoader,\n",
        "    torch.utils.data.DataLoader,\n",
        "    List[str]\n",
        "  ]:\n",
        "\n",
        "  train_data = datasets.ImageFolder(root=train_dir, transform=transforms)\n",
        "  test_data = datasets.ImageFolder(root=test_dir, transform=transforms)\n",
        "\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  train_dataloader = DataLoader(\n",
        "                                dataset=train_data,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=True,\n",
        "                                num_workers=num_workers,\n",
        "                                pin_memory=True\n",
        "                              )\n",
        "  test_dataloader = DataLoader(\n",
        "                                dataset=test_data,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=num_workers,\n",
        "                                pin_memory=True\n",
        "                              )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_test_model.py\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "def train_model(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    optimizer: torch.nn.Module\n",
        ") -> Tuple[float, float]:\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss, train_acc= 0, 0\n",
        "\n",
        "  for X, y in train_dataloader:\n",
        "\n",
        "    train_pred = model(X)\n",
        "\n",
        "    loss = loss_fn(train_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    accuracy = accuracy_score(\n",
        "        y,\n",
        "        torch.argmax(torch.softmax(train_pred, dim=1), dim=1)\n",
        "    )\n",
        "    train_acc += accuracy\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(train_dataloader)\n",
        "  train_acc /= len(train_dataloader)\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_model(\n",
        "    model: torch.nn.Module,\n",
        "    test_dataloader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module\n",
        ") -> Tuple[float, float]:\n",
        "\n",
        "  test_loss, test_acc= 0, 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "\n",
        "    for X, y in test_dataloader:\n",
        "\n",
        "      test_pred = model(X)\n",
        "\n",
        "      loss = loss_fn(test_pred, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      accuracy = accuracy_score(\n",
        "          y,\n",
        "          torch.argmax(torch.softmax(test_pred, dim=1), dim=1)\n",
        "      )\n",
        "      test_acc += accuracy\n",
        "\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train_test_model(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    test_dataloader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    optimizer: torch.nn.Module,\n",
        "    epochs: int = 5\n",
        ") -> Dict[str, float]:\n",
        "\n",
        "  results = {\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    train_loss, train_acc = train_model(model, train_dataloader, loss_fn, optimizer)\n",
        "    test_loss, test_acc = test_model(model, test_dataloader, loss_fn)\n",
        "\n",
        "    print(\n",
        "        \"-------------------------------------\\n\"\n",
        "        f\"Epoch: {epoch+1} |\\n\"\n",
        "        f\"Train Loss: {train_loss:.4f} |\\n\"\n",
        "        f\"Train Acc: {(train_acc * 100):.4f}% |\\n\"\n",
        "        f\"Test Loss: {test_loss:.4f} |\\n\"\n",
        "        f\"Test Acc: {(test_acc * 100):.4f}%\\n\"\n",
        "        \"-------------------------------------\\n\"\n",
        "    )\n",
        "\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G21WpPYGXrKd",
        "outputId": "658a036f-2d38-4e88-d395-49619c3c8fb3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_test_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(\n",
        "    model: torch.nn.Module,\n",
        "    target_dir: str,\n",
        "    model_name: str\n",
        ") -> None:\n",
        "\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  if not (model_name.endswith(\".pth\") or model_name.endswith(\".pt\")):\n",
        "    model_name += \".pth\"\n",
        "\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  print(f\"Saving model state dictionary to {model_save_path}\")\n",
        "\n",
        "  torch.save(obj=model.state_dict(), f=model_save_path)\n",
        "\n",
        "def set_seed(seed_to_set: int = 42):\n",
        "\n",
        "  torch.manual_seed(seed=seed_to_set)\n",
        "  torch.cuda.manual_seed(seed=seed_to_set)"
      ],
      "metadata": {
        "id": "l0IsNyh2Yavb",
        "outputId": "129df0eb-9dc2-4d79-a67a-7db3a3acace1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    }
  ]
}